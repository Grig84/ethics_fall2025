---
title: 'Blog Post 1: Teens Using AI as Therapists'
date: 2025-09-16
permalink: /posts/2025/16/blog-post-1/
tags:
  - case study
  - ethics
---

A summary and highlight of the current situation concerning Teens and their use of AI chatbots to replace mental health professionals.

**News Article:**  
[Teens Are Using Chatbots As Therapists. That's Alarming.](https://www.nytimes.com/2025/08/25/opinion/teen-mental-health-chatbots.html)

Discussion
---
This article stuck out to me because of the significance of the problem. We have a rising suicide rate among teens [(Source)](https://invisiblechildren.org/2025/05/31/suicides-among-children/), and chatbots provide both a potential solution and a potential aggregator for the problem. I was interested in seeing what affect these AI models have.

The ethical concern of this article is the unregulated nature of the responces of these AI chatbots. As teens seek more and more help and companionship from AI (72% of teens use them as companions), the lack of safeguards and regulation of what responces are suitable to suicide/broader mental health discussions has lead to chatbots offering advice on how to "safely" cut yourself, and how to harm yourself and get away with it. At the same time, the ease of access makes it much easeir for people to get some kind of help, which would lower the amount of americans w/mental health needs who recieve no support (1/2 of affected teens recieved no treatment last year). Another interesting ethical concern is if the freedom of speech applies to these chatbots or not.


Potential Stakeholders:

- **Therapists**: Want to maintain their job, and help the biggest number of people possible. AI models could take some of the load off their plate, allowing them more time to dedicate to the more delicate situations. At the same time, AI models could provide incorrect information, creating more work for therapists.
- **Affected Teens**: Want to gain easy and affective mental health support. AI models could provide private, easy and cheap support, allowing them to get the help they need when they need it. AI models could also provide an easier way for them to hurt themselves and worsen their problems.
- **AI companies**: Want the most users, and good public reception of their models. A suicide due to their chatbot could set them back immensely, and so they want to do everything in their power to protect their image and remove harmful responces.
- **Families of Teens**: Want the best for their teen. They want their teen to have access to safe and effective support.
- **Government/Regulatory bodies**: Want the best for their country/region, and want to regulate the models to ensure effective and safe therapy practices while maintaining the principles their region stands on, such as freedom of speech.


Ethical Frameworks:

The proposed solutions include connecting teens discussing mental health issues with chatbots to real therapists, regulating the use of chatbots, and implementing more studies to determine how chatbots will react to these statements. The **Care** framework supports connecting teens to therapists, as it will provide the best, most personal care to these teens. The **Duty** and **Contractarianism** frameworks would be against regulating AI use and responces, as they want to support the laws in the US granting freedoms, such as the freedom of speech. **Utalitarianism**, on the other hand, probably is for the regulation of chatbots, as it will prevent the speading of bad & harmful advice.


Reflection:

This is an interesting way to read and think about an article, and I found it quite fun. It takes more time than most reading responces I have had to do, but I am thinking very differently about this article and chatbots in general after writing this.
