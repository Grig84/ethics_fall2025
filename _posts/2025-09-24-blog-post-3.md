---
title: "Blog Post 3: Algorithms 1"
date: 2025-09-24
permalink: /posts/2025/16/blog-post-3/
tags:
  - case study
  - ethics
  - blog 3
  - discussion
  - question
  - algorithm
---

_____

**News Article:**  

[The Right to Be an Exception To a Data-Driven Rule](https://mit-serc.pubpub.org/pub/right-to-be-exception/release/2)

**Purpose of the Case Study:**

The purpose of this case study was to shed light on the problem that is the lack of awknolegement and inclusion of the outliers in data driven rules. Data-driven rules are good at predicting and classifiying the average data, but any time you get an outlier, they are treated poorly and not considered. Because of this, they propose new guidelines for using and trusting these data-driven methods that protect the individuality and nuance needed in the edge cases.

**Question Answers:**

1. A data-driven rule is a rule that decides things based solely on data, and an algorithm that chooses an outcome based on the same parameters for each input. An exception is an entry that has more nuanced features that do not exist in the standard set, leading to less accurate predictions. The algorithm is operating as it should, so it is not an error, but simply an oversight.

2. I think a big factor that was not listed above is the belief that humans hold that everyone is better than the sum of their parts. That leads to more leniancy (generally) when humans are making decions, and more nuanced factors getting in.

3. Individualization brings forth data privacy and model efficiency concerns, when we are adding more and more individualized training data, it has to come from somewhere. In the same vain, it would make the personalizations on every decion more accurate, so it can be used for more operations given our proposed limit on uncertainty.

4. Uncertianty allows for awknoledgement of the potential for outliers, and allows us to understand how effective the proposed model will be. In the same vain, if the accuracy of a model is truly 100% on all the population, and the training set is diverse, that model doesn't need uncertainty, since it is correct the WHOLE time. Otherwise, we need that uncertainty to understand how the model will perform, and how much to trust it.

**My Question & Why:**

Q: Do you see any potential issues in adopting these rules? How do they make it harder to make these decisions?

I came up with this question because I thought that they provided a good solution, but I don't think it is the only answer, and I think that it could have adverse effects on the efficiency and ease of using these techniques. I want to explore more how this would be implemented in general, because that was not looked into very much in the reading itself.

**Reflection:**

This case study worried me with its length, but it ended up being an easier read than I thought, and I actually ended up enjoying it. It was somewhat hard to create the question, but it also allowed me to think about the context deeper.
